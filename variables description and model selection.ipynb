{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39e9a4b-430e-4366-84af-5d594f1ba422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', '4_peak_mean', '4_non_mean', '7_peak_mean', '7_non_mean',\n",
       "       'non_peak_4_wd', 'peak_4_wd', 'non_peak_4_we', 'peak_4_we',\n",
       "       'non_peak_7_wd', 'peak_7_wd', 'non_peak_7_we', 'peak_7_we',\n",
       "       'bus distance', 'POI diversity', 'NDVI', 'road density',\n",
       "       'building density', 'BVI', 'SVI', 'GVI', 'VNMI', 'VHI',\n",
       "       'metro distance', 'slope', 'AQI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_T = pd.read_csv('./data/weather/heat.csv')\n",
    "df_BSU = pd.read_csv('./data/processed_bsu/combined_peak_non_peak.csv')\n",
    "df_v = pd.read_csv('./data/variables/variables_constant.csv')\n",
    "df_panel = df_T.merge(df_BSU, on='ID', how='inner').merge(df_v, on='ID', how='inner')\n",
    "df_panel[['non_peak_4_wd', 'non_peak_4_we', 'non_peak_7_wd', 'non_peak_7_we']] /= 7\n",
    "df_panel[['peak_7_we', 'peak_4_wd', 'peak_4_we', 'peak_7_wd']] /= 6\n",
    "df_panel.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedbec54-a92a-4294-9ee9-7f792845077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      POI diversity  4_peak_mean  4_non_mean  7_peak_mean  7_non_mean  \\\n",
      "mean       0.671678    28.369484   57.046532    63.689565   72.653198   \n",
      "std        0.629833     2.188049    2.469380     4.959599    4.406289   \n",
      "\n",
      "      non_peak_4_wd  peak_4_wd  non_peak_4_we  peak_4_we  non_peak_7_wd  \\\n",
      "mean      15.683505  35.410046      24.009637  30.562393      10.329242   \n",
      "std       20.629254  46.412591      29.974163  39.129642      13.408453   \n",
      "\n",
      "      peak_7_wd  non_peak_7_we  peak_7_we  \n",
      "mean  24.639040      10.918400  16.185897  \n",
      "std   32.101697      14.098195  20.556886  \n"
     ]
    }
   ],
   "source": [
    "columns_to_analyze = ['POI diversity',\n",
    "    '4_peak_mean', '4_non_mean', '7_peak_mean', '7_non_mean',\n",
    "    'non_peak_4_wd', 'peak_4_wd', 'non_peak_4_we', 'peak_4_we',\n",
    "    'non_peak_7_wd', 'peak_7_wd', 'non_peak_7_we', 'peak_7_we'\n",
    "]\n",
    "\n",
    "# 计算均值和标准差\n",
    "stats_df = df_panel[columns_to_analyze].agg(['mean', 'std'])\n",
    "\n",
    "# 打印结果\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbe74dc-5338-45e0-8eda-3a218ec2586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要分配的基础列\n",
    "common_cols = [\n",
    "    'bus distance', 'POI diversity', 'NDVI', 'road density',\n",
    "    'building density', 'BVI', 'SVI', 'GVI', 'VNMI', 'VHI',\n",
    "    'metro distance', 'slope', 'AQI'\n",
    "]\n",
    "\n",
    "# 定义每个子 DataFrame 的组合\n",
    "df_list = [\n",
    "    df_panel[['4_peak_mean', 'peak_4_wd'] + common_cols].rename(\n",
    "        columns={'4_peak_mean': 'Tmrt', 'peak_4_wd': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['4_peak_mean', 'peak_4_we'] + common_cols].rename(\n",
    "        columns={'4_peak_mean': 'Tmrt', 'peak_4_we': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['7_peak_mean', 'peak_7_wd'] + common_cols].rename(\n",
    "        columns={'7_peak_mean': 'Tmrt', 'peak_7_wd': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['7_peak_mean', 'peak_7_we'] + common_cols].rename(\n",
    "        columns={'7_peak_mean': 'Tmrt', 'peak_7_we': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['4_non_mean', 'non_peak_4_we'] + common_cols].rename(\n",
    "        columns={'4_non_mean': 'Tmrt', 'non_peak_4_we': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['4_non_mean', 'non_peak_4_wd'] + common_cols].rename(\n",
    "        columns={'4_non_mean': 'Tmrt', 'non_peak_4_wd': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['7_non_mean', 'non_peak_7_wd'] + common_cols].rename(\n",
    "        columns={'7_non_mean': 'Tmrt', 'non_peak_7_wd': 'bike-sharing usage'}\n",
    "    ),\n",
    "    df_panel[['7_non_mean', 'non_peak_7_we'] + common_cols].rename(\n",
    "        columns={'7_non_mean': 'Tmrt', 'non_peak_7_we': 'bike-sharing usage'}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 可通过 df_list[0], df_list[1], ... 访问各个子 DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc1224d-d1d4-4375-936c-f5affb9d939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义导出文件名（使用对应列名进行命名）\n",
    "file_names = [\n",
    "    'peak_4_wd.csv', 'peak_4_we.csv',\n",
    "    'peak_7_wd.csv', 'peak_7_we.csv',\n",
    "    'non_peak_4_we.csv', 'non_peak_4_wd.csv',\n",
    "    'non_peak_7_wd.csv', 'non_peak_7_we.csv'\n",
    "]\n",
    "\n",
    "# 导出为 CSV 文件\n",
    "for i, df in enumerate(df_list):\n",
    "    df.to_csv(f'./data/variables/{file_names[i]}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc6c5322-a78d-4fe8-bb84-328969be7ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./data/panel/peak_7_wd.csv\n",
      "Processing file: ./data/panel/peak_7_we.csv\n",
      "Processing file: ./data/panel/non_peak_4_we.csv\n",
      "Processing file: ./data/panel/non_peak_4_wd.csv\n",
      "Processing file: ./data/panel/peak_4_we.csv\n",
      "Processing file: ./data/panel/peak_4_wd.csv\n",
      "Processing file: ./data/panel/non_peak_7_wd.csv\n",
      "Processing file: ./data/panel/non_peak_7_we.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# 定义目录路径\n",
    "plot_dir = './data/panel/'\n",
    "\n",
    "# 获取所有 CSV 文件\n",
    "csv_files = [f for f in os.listdir(plot_dir) if f.endswith('.csv')]\n",
    "\n",
    "# 初始化结果字典\n",
    "results = {\n",
    "    'Model': ['OLS', '', 'GBR', '', 'MLP', '', 'XGB', ''],\n",
    "}\n",
    "\n",
    "# 循环读取 CSV 文件并进行建模\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(plot_dir, file)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # 加载数据\n",
    "    dfj = pd.read_csv(file_path)\n",
    "\n",
    "    # 删除 'bike-sharing usage' 为0的行\n",
    "    dfj = dfj[dfj['bike-sharing usage'] > 0]\n",
    "\n",
    "    # 计算 log_bike-sharing usage 列\n",
    "    dfj['log_bike-sharing usage'] = np.log(dfj['bike-sharing usage'])\n",
    "\n",
    "    # 定义自变量\n",
    "    X = dfj[['Tmrt','POI diversity','road density', 'building density', 'bus distance', 'metro distance', 'BVI', 'SVI', 'GVI', \n",
    "             'VNMI', 'VHI', 'slope','NDVI' ,'AQI']]\n",
    "\n",
    "    # 定义因变量\n",
    "    y = dfj['log_bike-sharing usage']\n",
    "\n",
    "    # 标准化自变量，并确保索引对齐\n",
    "    scaler = StandardScaler()\n",
    "    X_standardized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    # 重置索引，保持自变量和因变量索引一致\n",
    "    y = y.reset_index(drop=True)\n",
    "    X_standardized = X_standardized.reset_index(drop=True)\n",
    "\n",
    "    # 1. OLS回归\n",
    "    X_with_const = add_constant(X_standardized)\n",
    "    ols_model = sm.OLS(y, X_with_const).fit()\n",
    "    ols_pred = ols_model.predict(X_with_const)\n",
    "    ols_r2 = r2_score(y, ols_pred)\n",
    "    ols_mse = mean_squared_error(y, ols_pred)\n",
    "\n",
    "    # 2. MLP回归\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "    mlp_model.fit(X_standardized, y)\n",
    "    mlp_pred = mlp_model.predict(X_standardized)\n",
    "    mlp_r2 = r2_score(y, mlp_pred)\n",
    "    mlp_mse = mean_squared_error(y, mlp_pred)\n",
    "\n",
    "    # 3. GBR回归\n",
    "    gbr_model = GradientBoostingRegressor(n_estimators=20, random_state=42)\n",
    "    gbr_model.fit(X_standardized, y)\n",
    "    gbr_pred = gbr_model.predict(X_standardized)\n",
    "    gbr_r2 = r2_score(y, gbr_pred)\n",
    "    gbr_mse = mean_squared_error(y, gbr_pred)\n",
    "\n",
    "    # 4. XGBoost回归\n",
    "    xgb_model = XGBRegressor(n_estimators=20, random_state=42)\n",
    "    xgb_model.fit(X_standardized, y)\n",
    "    xgb_pred = xgb_model.predict(X_standardized)\n",
    "    xgb_r2 = r2_score(y, xgb_pred)\n",
    "    xgb_mse = mean_squared_error(y, xgb_pred)\n",
    "\n",
    "    # 将结果存入结果字典\n",
    "    results[file] = [\n",
    "        ols_r2, ols_mse,\n",
    "        gbr_r2, gbr_mse,\n",
    "        mlp_r2, mlp_mse,\n",
    "        xgb_r2, xgb_mse\n",
    "    ]\n",
    "\n",
    "# 将结果转换为 DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# # 显示结果\n",
    "# import ace_tools as tools\n",
    "# tools.display_dataframe_to_user(name=\"Model Results\", dataframe=results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36f95e0f-e687-4f25-af5b-7e58a9a7a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./data/panel/model.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab98198-1ff3-4c6a-b9fd-4dbc5c9078e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
